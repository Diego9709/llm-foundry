data_local: /mnt/ceph-dataset/nvme/zhangkang/llama_shard

data_remote:                              # If blank, files must be present in data_local
max_seq_len: 4096
global_seed: 17         # org = 17
precision: FP32
dist_timeout: 1200.00

# Run Name
run_name:                                 # If left blank, will be read from env var $RUN_NAME

# Model
model:
  name: llama_causal_lm
  init_device: cpu
  hidden_size: 512
  num_attention_heads: 4
  num_hidden_layers: 4
  expansion_ratio: 4
  intermediate_size:
  hidden_act: silu 
  max_position_embeddings: ${max_seq_len}
  use_cache: True
  vocab_size: 130344
  pad_token_id: 3
  bos_token_id: 130004
  eos_token_id: 130005
  pretraining_tp: 1
  tie_word_embeddings: Flase
  rope_theta: 10000.0
  rope_scaling:
    type: dynamic
    factor: 1.1
  attention_bias: false
  attention_dropout: 0.0
  loss_fn: torch_crossentropy





# Tokenizer
tokenizer:
  name: /mnt/ceph-dataset/zhangkang/work/llama/llama/tokenizer/
  kwargs:
    model_max_length: ${max_seq_len}
    trust_remote_code: true


# Optimization
scheduler:
  name: cosine_with_warmup
  t_warmup: 100ba
  alpha_f: 0.1                # default = 0.1

optimizer:
  name: decoupled_adamw         # ====== default = decoupled_adamw
  lr: 1.2e-4                    # default =  1.2e-4, tried: 3e-4
  betas:
  - 0.9
  - 0.95
  eps: 1.0e-08                  # ====== default 1.0e-08
  weight_decay: 0.00             # ====== default = 0

algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 0.3       # 1.0

max_duration: 1ep           # (1) init set to 160000 (2) at 64250ba change to 100000.
eval_interval: 0                # 5000ba
eval_first: false
eval_subset_num_batches: -1
global_train_batch_size: 12   # default = 1024 = Nodes * 8 * gradient acc steps * device_train_microbatch_size



# System
seed: ${global_seed}
device_eval_batch_size: 32              # default = 8
device_train_microbatch_size: 32        # default = 8
# device_train_microbatch_size: auto


# FSDP
fsdp_config:
  sharding_strategy: FULL_SHARD       # default = FULL_SHARD, can also be: SHARD_GRAD_OP
  mixed_precision: PURE
  state_dict_type: full                 # ============= Added ===============
  activation_checkpointing: true
  activation_checkpointing_reentrant: false
  activation_cpu_offload: false
  limit_all_gathers: true
  verbose: false

# Logging
progress_bar: false
log_to_console: true
console_log_interval: 1ba


# Dataloaders
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train                    # phase1: train_small; phase2: p1
    shuffle: true
    max_seq_len: ${max_seq_len}
    shuffle_seed: ${global_seed}
  drop_last: true
  num_workers: 8

eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: val_small
    shuffle: false
    max_seq_len: ${max_seq_len}
    shuffle_seed: ${global_seed}
  drop_last: false
  num_workers: 8


callbacks:
  speed_monitor:
    window_size: 10
  lr_monitor: {}
  memory_monitor: {}
  runtime_estimator: {}

# save_interval: 1ep
save_interval: 250ba

# Checkpoint to local filesystem or remote object store
# save_interval: 5000ba
# save_num_checkpoints_to_keep: 10  # Important, this cleans up checkpoints saved to DISK
save_folder: /mnt/ceph-dataset/zhangkang/llama_ckpt/composer_models/${run_name}
   # init lr = 1.2e-5
load_path:   # init lr = 1.2e-5 AND reset optimizer


